{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7LrchFUHo3eu"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar Librerias"
      ],
      "metadata": {
        "id": "7LrchFUHo3eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs_ds           # bs_ds (Beautiful Soup Data Science)\n",
        "!pip install fake_useragent  # fake_useragent se utiliza para generar encabezados de agente de usuario falsos (user agents) de manera aleatoria.\n",
        "!pip install lxml            # lxml es una biblioteca de procesamiento de XML y HTML.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNtt49HuWctp",
        "outputId": "1bf54c5e-3715-49dd-ea60-0142e57173e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs_ds\n",
            "  Downloading bs_ds-0.11.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/84.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from bs_ds) (7.34.0)\n",
            "INFO: pip is looking at multiple versions of bs-ds to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading bs_ds-0.11.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.10.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from bs_ds) (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bs_ds) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.2.2)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.11.4)\n",
            "Collecting shap (from bs_ds)\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting LIME (from bs_ds)\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs_ds\n",
            "  Downloading bs_ds-0.9.12-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.11-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.10-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.9-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.8-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of bs-ds to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading bs_ds-0.9.7-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.6-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/80.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.5-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.4-py2.py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.3-py2.py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading bs_ds-0.9.2-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.9.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.9-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.7-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.6-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.5-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.4-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m901.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.3-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.2-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.8.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading bs_ds-0.7.4-py2.py3-none-any.whl (35 kB)\n",
            "  Downloading bs_ds-0.7.3-py2.py3-none-any.whl (35 kB)\n",
            "  Downloading bs_ds-0.7.2-py2.py3-none-any.whl (34 kB)\n",
            "  Downloading bs_ds-0.7.1-py2.py3-none-any.whl (34 kB)\n",
            "  Downloading bs_ds-0.7.0-py2.py3-none-any.whl (34 kB)\n",
            "  Downloading bs_ds-0.6.6-py2.py3-none-any.whl (34 kB)\n",
            "  Downloading bs_ds-0.6.5-py2.py3-none-any.whl (33 kB)\n",
            "  Downloading bs_ds-0.6.3-py2.py3-none-any.whl (39 kB)\n",
            "  Downloading bs_ds-0.6.2-py2.py3-none-any.whl (38 kB)\n",
            "  Downloading bs_ds-0.6.1-py2.py3-none-any.whl (38 kB)\n",
            "  Downloading bs_ds-0.6.0-py2.py3-none-any.whl (38 kB)\n",
            "  Downloading bs_ds-0.5.4-py2.py3-none-any.whl (38 kB)\n",
            "  Downloading bs_ds-0.5.3-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.0.3)\n",
            "  Downloading bs_ds-0.5.2-py2.py3-none-any.whl (29 kB)\n",
            "  Downloading bs_ds-0.5.1-py2.py3-none-any.whl (29 kB)\n",
            "  Downloading bs_ds-0.5.0-py2.py3-none-any.whl (29 kB)\n",
            "  Downloading bs_ds-0.4.12-py2.py3-none-any.whl (29 kB)\n",
            "  Downloading bs_ds-0.4.11-py2.py3-none-any.whl (29 kB)\n",
            "  Downloading bs_ds-0.4.10-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading bs_ds-0.4.9-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading bs_ds-0.4.8-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading bs_ds-0.4.7-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading bs_ds-0.4.6-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading bs_ds-0.4.5-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading bs_ds-0.4.4-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading bs_ds-0.4.3-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading bs_ds-0.4.2-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading bs_ds-0.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading bs_ds-0.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading bs_ds-0.3.12-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from bs_ds) (8.1.7)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (67.7.2)\n",
            "Collecting jedi>=0.16 (from IPython->bs_ds)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bs_ds) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bs_ds) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bs_ds) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bs_ds) (3.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->bs_ds) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->bs_ds) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->bs_ds) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->bs_ds) (1.16.0)\n",
            "Installing collected packages: jedi, bs_ds\n",
            "Successfully installed bs_ds-0.3.12 jedi-0.19.1\n",
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.5.1\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerias a utilizar"
      ],
      "metadata": {
        "id": "e6QBOnZ8opPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BeautifulSoup: Analiza HTML/XML para extraer datos.\n",
        "from bs4 import BeautifulSoup\n",
        "# requests: Realiza solicitudes HTTP para obtener contenido web.\n",
        "import requests\n",
        "# urljoin: Construye URLs completas a partir de enlaces.\n",
        "from urllib.parse import urljoin\n",
        "# se utiliza para guardar el archivo final en google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# libreria necesaria para crear un archivo CSV\n",
        "import csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmc48q11agTI",
        "outputId": "7ed363ac-899c-408a-c059-778846ea7e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realizar solicitud HTTP"
      ],
      "metadata": {
        "id": "MU5Efu7VpGYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL a la que se accederá\n",
        "url = 'https://www.informador.mx/seccion/ultimas-noticias'\n",
        "\n",
        "# Envía una solicitud HTTP con tiempo de espera de 3 segundos\n",
        "response = requests.get(url, timeout=3)\n",
        "\n",
        "# Imprime el código de estado de la respuesta\n",
        "print('Código de estado HTTP: ', response.status_code)\n",
        "\n",
        "# Condicional que indica una conexión exitosa (200 OK) si no imprime un mensaje en consecuencia\n",
        "if response.status_code == 200:\n",
        "    print(f\"{'--'*20}\\n\\tConexión exitosa OK.\\n{'--'*20}\\n\")\n",
        "else:\n",
        "    print('Error.\\n\\n')\n",
        "\n",
        "# Imprime el contenido de la respuesta de la solicitud\n",
        "print(f\"{'***'*20}\\n\\tRespuesta de la solicitud:\\n{'***'*20}\")\n",
        "\n",
        "# Informacion de los encabezados de la respuesta HTTP recibida del servidor.\n",
        "for k, v in response.headers.items():\n",
        "    print(f\"{k:{30}}: {v:{40}}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je67UVMmXT4o",
        "outputId": "f47f5db0-741a-485f-9d75-38557f7eacd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Código de estado HTTP:  200\n",
            "----------------------------------------\n",
            "\tConexión exitosa OK.\n",
            "----------------------------------------\n",
            "\n",
            "************************************************************\n",
            "\tRespuesta de la solicitud:\n",
            "************************************************************\n",
            "Content-Type                  : text/html;charset=UTF-8                 \n",
            "Transfer-Encoding             : chunked                                 \n",
            "Connection                    : keep-alive                              \n",
            "Date                          : Mon, 29 Apr 2024 16:10:14 GMT           \n",
            "Access-Control-Allow-Origin   : *                                       \n",
            "Last-Modified                 : Mon, 29 Apr 2024 16:10:14 GMT           \n",
            "Content-Encoding              : gzip                                    \n",
            "X-Cacheable                   : YES                                     \n",
            "Cache-Control                 : max-age=300, public, stale-if-error=84600, stale-while-revalidate=120\n",
            "X-Cacheable-TTL               : 60s                                     \n",
            "Cache-tag                     : html                                    \n",
            "X-Cache-Hits                  : 0                                       \n",
            "Accept-Ranges                 : bytes                                   \n",
            "Vary                          : Accept-Encoding                         \n",
            "X-Cache                       : Miss from cloudfront                    \n",
            "Via                           : 1.1 eec312af4858d4158abb9119dac71d94.cloudfront.net (CloudFront)\n",
            "X-Amz-Cf-Pop                  : MIA3-P6                                 \n",
            "Alt-Svc                       : h3=\":443\"; ma=86400                     \n",
            "X-Amz-Cf-Id                   : RrA-gteGrrbDRZKNueBbxpJQyLPxBLvQfh-SA5NedVkoP0UXr8cOTg==\n",
            "Age                           : 0                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VLRRxvPRNC7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta al archivo CSV en tu Google Drive\n",
        "file_path = '/content/drive/MyDrive/scrap/texto_extraido.csv'\n",
        "# Realiza una solicitud HTTP GET a la URL\n",
        "response = requests.get(url)\n",
        "# Extrae el contenido HTML de la respuesta\n",
        "content = response.text\n",
        "# Crea un objeto BeautifulSoup para analizar el contenido HTML\n",
        "soup = BeautifulSoup(content, 'lxml')\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Encuentra titulos, fechas ,secciones de las Ultimas noticias.\n",
        "# para esto es necesario inspeccionar la pagina y obtener las partes requeridas\n",
        "t_encabezado = soup.find('h2', class_='mod-title')\n",
        "news_titles = soup.find_all('h2', class_='news-title')\n",
        "news_dates = soup.find_all('time', class_='news-date')\n",
        "news_sections = soup.find_all('span', class_='news-section')\n",
        "\n",
        "# Obtiene el texto del encabezado\n",
        "texto_h2 = t_encabezado.text\n",
        "print(f\"{'---'*20}\\n{texto_h2}\\n{'---'*20}\\n\")\n",
        "\n",
        "\n",
        "# Abrir el archivo CSV para escribir la cabecera si es la primera iteración\n",
        "with open(file_path, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Fecha', 'Seccion', 'Noticia', 'URL', 'Sinopsis', 'autor'])\n",
        "\n",
        "# iterar entre las variables anteriores\n",
        "for i, (news_date, news_section, news_title) in enumerate(zip(news_dates, news_sections, news_titles), start=1):\n",
        "\n",
        "    # Muestra el contenido del atributo datetime de <time> (fecha)\n",
        "    date_text = news_date['datetime']\n",
        "\n",
        "    # Muestra el texto de <span> (seccion)\n",
        "    section_text = news_section.get_text(strip=True)\n",
        "    cleaned_text = ' '.join(section_text.split())\n",
        "    #muestra el numero de iteraciones\n",
        "    print(f\"Numero Noticia {i} \")\n",
        "    print(f\"{'***'*20}\\nSección: {cleaned_text}\")\n",
        "\n",
        "    # Extraer el enlace de redireccion q hay dentro del titulo\n",
        "\n",
        "    # Encuentra el elemento <a> dentro del elemento h2 actual\n",
        "    link_element = news_title.find('a')\n",
        "    # Obtiene el enlace (href) y el texto del elemento <a>\n",
        "    link_href = link_element.get('href')\n",
        "    #muestra el texto de titulo de la noticia\n",
        "    link_text = link_element.get_text(strip=True)\n",
        "\n",
        "    #unir la url con el link del titulo de la noticia\n",
        "    full_url = urljoin(url, link_href)\n",
        "\n",
        "    #muestra resultados\n",
        "    print(f\"Noticia: {link_text}\")\n",
        "    print(f\"Fecha: {date_text}\")\n",
        "    print(f\"URL: {full_url}\\n\")\n",
        "\n",
        "    #obtener el nombre autor q esta dentro del link de cada noticia\n",
        "    # Realiza una solicitud HTTP a la URL completa\n",
        "    linked_page_response = requests.get(full_url)\n",
        "\n",
        "    # Crea un objeto BeautifulSoup para analizar el contenido de la página vinculada\n",
        "    linked_page_soup = BeautifulSoup(linked_page_response.content, 'lxml')\n",
        "\n",
        "    # Encuentra autores dentro de la pagina\n",
        "    linked_page_content = linked_page_soup.find('p', class_='news-author')\n",
        "    #linked_page_content_text = linked_page_content.get_text(strip=True).replace('por', '')\n",
        "\n",
        "    # Imprime el contenido de la página vinculada\n",
        "    print(f\"Contenido del URL de la noticia:\\n{linked_page_content.get_text(strip=True)}\\n{'***' * 20}\\n\")\n",
        "\n",
        "    #extra mostra primer parrafo de la noticia\n",
        "    parrafo = linked_page_soup.find('div', class_='news-excerpt')\n",
        "\n",
        "    # Imprime el contenido de la página vinculada\n",
        "    print(f\"{'▒▒▒'*20 }\\n\\tSinopsis:\\n{parrafo.get_text(strip=True)}\\n\\n{'▒▒▒'* 20}\\n\\n\")\n",
        "\n",
        "    formatted_date = date_text.replace(\"T\", \" \")\n",
        "\n",
        "# Convertir la cadena a un objeto datetime\n",
        "\n",
        "    fecha_objeto = datetime.strptime(date_text, \"%Y-%d-%mT%H:%M\")\n",
        "\n",
        "\n",
        "# Formatear la fecha y hora en otro formato ISO 8601\n",
        "    fecha_iso8601_otro_formato = fecha_objeto.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "   # Escribe los datos en el archivo CSV\n",
        "    with open(file_path, 'a', newline='') as file:\n",
        "     writer = csv.writer(file)\n",
        "     writer.writerow([fecha_objeto, cleaned_text, link_text, full_url, parrafo.get_text(strip=True), linked_page_content.get_text(strip=True)[4:]])"
      ],
      "metadata": {
        "id": "PYAJ-xArnezy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQ7ge_ZvpIFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}